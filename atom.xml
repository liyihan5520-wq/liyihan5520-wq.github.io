<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liyihan5520-wq.github.io/</id>
    <title>YH&apos;s Blog</title>
    <updated>2025-11-26T14:26:52.688Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liyihan5520-wq.github.io/"/>
    <link rel="self" href="https://liyihan5520-wq.github.io/atom.xml"/>
    <subtitle>A junior programmer, focusing on Agent and LLM.     Keep learning and sharing knowledge of  AI</subtitle>
    <logo>https://liyihan5520-wq.github.io/images/avatar.png</logo>
    <icon>https://liyihan5520-wq.github.io/favicon.ico</icon>
    <rights>All rights reserved 2025, YH&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Agentic Design Patterns]]></title>
        <id>https://liyihan5520-wq.github.io/post/agentic-design-patterns/</id>
        <link href="https://liyihan5520-wq.github.io/post/agentic-design-patterns/">
        </link>
        <updated>2025-11-25T14:22:22.000Z</updated>
        <content type="html"><![CDATA[<p>Key Components: Perception, Planning, Tooling, Memory Management, Action Selection<br>
Key insight: Proper Agent design depends on the task</p>
<p>目前梳理出的Agent设计模式有 9 种，分别为：ReAct/ Plan and Solve/ Reason without Observation/ LLMCompiler/ Basic Reflection/ Reflexion / Language Agent Tree Search</p>
<p>Google 团队推出的 《Agentic design patterns》 更加详细地阐述了从基础到高级的设计理念与实现方法  https://github.com/xindoo/agentic-design-patterns</p>
<h1 id="react-pattern">ReAct Pattern:</h1>
<ul>
<li>模仿人类解决问题的过程，将单轮回答拆成多轮 TAO 循环： 思考（分析、规划） -&gt; 行动(执行工具调用/环境交互) -&gt;观察(环境变化)-&gt;再思考 循环往复</li>
<li>将 思考、行动、观察 三者解耦，得到事件驱动的，可解释、可调试、可扩展的通用 Agent 框架</li>
</ul>
<p>from langchain.agents import create_react_agent, AgentExecutor<br>
from langchain_openai import ChatOpenAI<br>
from langchain.tools import BaseTool<br>
class Calculator(BaseTool):<br>
name = &quot;calculator&quot;<br>
description = &quot;eval math&quot;<br>
def _run(self, expr: str):<br>
return eval(expr)<br>
llm   = ChatOpenAI(temperature=0)<br>
tools = [Calculator()]<br>
agent = create_react_agent(llm, tools, hub.pull(&quot;hwchase17/react&quot;))<br>
[把「LLM + 工具集合 + ReAct 提示模板」封装成一个 可序列化的 Runnable]<br>
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)<br>
[实现 TAO 循环调度器（Thought-Action-Observation）]</p>
<p><strong>优点</strong>： 动态适应性强，能实时根据环境观察灵活调整行动计划，其推理轨迹使得整个决策过程高度可解释，有利于开发者调试，增强用户信任度<br>
<strong>缺点</strong>：每次工具调用都需要进行一次LLM 推理，执行速度慢，token 消耗高，上下文难以管理，“走一步看一步”可能陷入局部最优<br>
<strong>适用场景</strong>：适合需要与外部环境进行持续交互的场景</p>
<h1 id="plan-execute">Plan &amp; Execute</h1>
<ul>
<li>将Agent 的工作流明确地分为两个独立阶段，一个由功能强大的LLM 负责“规划”阶段，和一个更轻量级或特定模型负责“执行”阶段，在规划阶段由功能强大的 LLM 一次性生成一个详细的、多步骤的静态计划，执行器根据计划逐步完成</li>
<li>牺牲React 的实时适应性，换来更高的执行效率和更低的运营成本。这种设计模式通过将 &quot;智慧（规划）&quot; 与“体力（执行）”解耦</li>
</ul>
<p><strong>优点</strong>： token 花费少，强制任务开始时进行全局思考，有助于避免ReAct 可能出现的局部最优问题<br>
<strong>缺点</strong>：鲁棒性差，计划是静态的，无法在执行过程中应对突发状况或错误<br>
<strong>适用场景</strong>：适用流程相对固定，但步骤繁多，需要调用多种工具的复杂任务</p>
<p>#Reason without Observation</p>
<ul>
<li>ReWOO, 无观察推理 Plan &amp; Execute 架构的高效变体，其核心理念在于， planner 一次性生成一个完整的、包含变量站位符的计划，如 #E1 = Tool[argument] 代表工具调用的输出，可以在后续步骤中直接引用，从而实现数据的高效传递</li>
<li>将编译原理中的“变量” 概念引入自然语言计划，实现更高效的数据流编排</li>
</ul>
<p><strong>优点</strong>：通过一次性规划整个任务链，避免了ReAct 中反复调用 LLM 所产生的冗余提示词和上下文，显著减少了 token 消耗<br>
<strong>缺点</strong>:  尽管引入了变量，其任务执行本质上仍是串行的，无法充分利用可以并行的任务，与 Plan &amp; Execute 类似<br>
<strong>使用场景</strong>:  适合需要链式调用工具来获取信息，且工具之间需要传递数据</p>
<h1 id="llm-compiler">LLM Compiler</h1>
<ul>
<li>让Planner 生成一个有向无环图（DAG），而非简单的列表，这个任务图清晰定义了所有任务、所需工具，以及任务间的依赖关系，独立的任务调度单元会根据这个 DAG 自动执行满足依赖的任务，从而实现最大化的并发执行，提供显著的速度提升</li>
</ul>
<p><strong>优点</strong>：能够实现极高的执行效率，节约成本，减少总体执行时间，任务图的输出便于理解和调试，提高了可解释性<br>
<strong>缺点</strong>：工程实现难度高，需要构建复杂的任务调度单元和依赖管理系统。单点中断可能导致整个流程失败<br>
<strong>适用场景</strong>：需要同时获取多项信息或调用多个 API 完成任务的场景</p>
<p>#Basic Reflection</p>
<ul>
<li>明确将 “反思”，作为Agent 工作流中的一个步骤，使模型能够思考其自身思考过程，并根据内在逻辑进行自我纠正</li>
</ul>
<p><strong>优点</strong>：即使简单的反思循环，能够在创造性/逻辑性任务中带来显著的性能提升，这种方法可以帮助LLM 减少输出中的偏见，提升安全性和<br>
<strong>缺点</strong>：缺少外部反馈，单纯让另一个模型去“挑刺”， Agent 可能会陷入低效的自我循环中，无法找到正确的解决方案<br>
<strong>适用场景</strong>: 需要迭代优化的创作或编程任务</p>
<p>#Reflexion</p>
<ul>
<li>引入评估器 evaluator 判断当前轨迹成功与否，由reflector 生成文本化反思，这段反思被“动态记忆”储存起来，并在下一次尝试中作用于上下文，指导Agent 避免重复过去错误</li>
</ul>
<p><strong>优点</strong>：其记忆是显示的反思文本，Agent 的“学习”过程透明可追踪，提供了更高的可解释性<br>
<strong>缺点</strong>：高度依赖于 LLM 能否准确评估自己的表现，并生成有用的反思，引入了记忆，同时也是对上下文管理的考验<br>
<strong>适用场景</strong>: 需要从失败中汲取教训以找到解决方案的复杂任务（强化学习/元学习）</p>
<h1 id="language-agent-tree-search">Language Agent Tree Search</h1>
<ul>
<li>将语言模型的推理能力与经典的“蒙特卡洛树搜索”算法结合，创造出一个能够探索多条可能路径并进行深度决策的框架。</li>
<li>将LLM 作为 价值函数和优化器，通过树状搜索同时探索多个 React 序列，并利用反思和外部反馈来评估和回溯最优路径</li>
</ul>
<p><strong>优点</strong>: 能够在复杂的决策空间进行深度探索，避免单一路径的失败。该框架在多种任务上均表现出优越性能，是一种综合可靠的强大框架<br>
<strong>缺点</strong> 为了保证效果，往往需要大量的 LLM 调用<br>
<strong>适用场景</strong>： 适合需要深度探索和权衡多种可能性的复杂决策任务</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prompt Engineerning]]></title>
        <id>https://liyihan5520-wq.github.io/post/prompt-engineerning/</id>
        <link href="https://liyihan5520-wq.github.io/post/prompt-engineerning/">
        </link>
        <updated>2025-11-24T12:03:19.000Z</updated>
        <content type="html"><![CDATA[<p>定义 Agent 功能的 System Prompt，用于设定 llm 整体行为，帮助模型了解用户需求，对于 Agent 能力表现至关重要，在此系统学习一下 Prompt Engineerning 相关技术</p>
<h1 id="1-chain-of-thoughts">1. chain of thoughts</h1>
<blockquote>
<p>鼓励大模型解释其推理过程，把一个多步推理问题分解成很多中间步骤，分配给更多计算量，生成更多的token，在将答案拼接在一起求解</p>
</blockquote>
<ul>
<li>zero-shot-CoT:     通常在问题结尾附加 &quot;Let's think step by step&quot;</li>
<li>self-consistency:   生成多个思路链，然后取多数答案作为最终答案</li>
<li>knowledge-generation prompt:  先引导模型生成问题相关知识，再将知识拼接到 prompt 中</li>
<li>prompt chaining:  将问题分解为多个子问题，根据子任务创建一系列提示操作</li>
<li>ToT:  思维树，能够自己对严谨推理过程的中间思维进行评估</li>
</ul>
<blockquote>
<p>假设三位不同的专家来回答这个问题。所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。<br>
然后，所有专家都写下他们思考的下一个步骤并分享。以此类推，直到所有专家写完他们思考的所有步骤。.只要大家发现有专家的步骤出错了，就让这位专家离开。<br>
请问...</p>
</blockquote>
<h1 id="2-self-refine">2. Self-Refine</h1>
<blockquote>
<p>考虑到LLM 第一次生成结果可能不是最好的输出，提出一种包括反馈(feedback) 和 改善(refinement)两个步骤的迭代方案，来改进LLM的初始输出</p>
</blockquote>
<h1 id="3-reflection">3. Reflection</h1>
<blockquote>
<p>引入反思机制，有助于帮助智能体快速从之前的错误中学习</p>
</blockquote>
<ul>
<li>参与者（Actor）：根据状态观测量生成文本和动作。参与者在环境中采取行动并接受观察结果，从而形成轨迹。</li>
<li>评估者（Evaluator）：对参与者的输出进行评价。具体来说，它将生成的轨迹（也被称作短期记忆）作为输入并输出奖励分数。</li>
<li>自我反思（Self-Reflection）：生成语言强化线索来帮助参与者实现自我完善。这个角色由大语言模型承担，能够为未来的试验提供宝贵的反馈。自我反思模型利用奖励信号、当前轨迹和其持久记忆生成具体且相关的反馈，并存储在记忆组件中。智能体利用这些经验（存储在长期记忆中）来快速改进决策</li>
</ul>
<h1 id="4-react">4. ReAct</h1>
<blockquote>
<p>引导模型将复杂问题进行拆解，一步一步地进行推理 (Reasoning) 和行动 (Action)，同时还引入了观察 (Observation) 环节，在每次执行(Action) 之后，都会先观察(Observation) 当前现状，然后进行下一步推理(Reason)</p>
</blockquote>
<ul>
<li>思维链 (Chain of Thought): 将一个大的复杂任务进行拆解，拆解成多个思维步骤</li>
<li>推理(Reasoning): 负责分析和处理输入的数据，生成有效的决策</li>
<li>行动(Action): 执行具体操作</li>
<li>观察(Observation): 监控和收集环境反馈数据，为下一步推理和行动提供依</li>
</ul>
<h1 id="2个准则">2个准则：</h1>
<ul>
<li>好的 prompt 不是一次调整好的</li>
<li>关于评判提示词好/坏, 最好有一个准则</li>
</ul>
<p>9个要点：</p>
<ol>
<li>让模型帮你生成 (Prompt generator)</li>
<li>写得尽可能清晰 (Be clear and direct)</li>
<li>给模型更多的例子 (Use examples)   zero-shot / one-shot / few-shot</li>
<li>给模型的思考空间 (Let Claude think (chain of thought))  [think it step by step]</li>
<li>使用XML标签      (Use XML tags)  [针对容易让模型带有缩进混乱的东西]</li>
<li>给模型角色       (Give Claude a role)</li>
<li>让模型直接补全    (Prefill Claude's response)   [让模型直接补全]</li>
<li>思维链式 prompt  (workflow: prompt1 -&gt; step1, prompt2 -&gt; step2, ...)</li>
</ol>
<h1 id="context-engineering"><strong>Context Engineering</strong></h1>
<blockquote>
<p>在Multi-agennt 或者多步任务中，中间过程会产生大量上下文信息，但这些上下文信息较为繁杂冗余，如果每次都将上下文“一股脑”打包到 prompt 中输入给LLM 十分占用 token 且可能混淆关键信息，上下文工程，如何通过一套程序化的规则，来自动管理和修改“上下文”， 确保 Agent 在 自主行动中始终符合用户的最初要求[Goal]</p>
</blockquote>
<ul>
<li>notebooker:  引入新的 tool/agent ，仅保存上下文中的关键信息，不再“一股脑”的将中间信息全部拼接起来【一般将笔记拼接到“首/尾”（相对于transformer 架构更加显眼）】</li>
<li>优化上下文长度</li>
<li>引入RAG，将上下文信息存入到向量数据库中</li>
</ul>
]]></content>
    </entry>
</feed>